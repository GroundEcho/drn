{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#首先安装paddlex\n",
    "! pip install paddlex -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验名称：用paddleX高阶API完成目标检测任务\n",
    "\n",
    "## 实验介绍：\n",
    "\n",
    "百度开放的PaddleX就是其中一个AI开放平台。PaddleX简单易用，且功能强大，不但适合个人的开发实践，也适合企业应用。而paddleX有两种模式，一种是可视化端模式，这种模式可以让我们零代码的完成一个目标检测任务。paddleX的另一种模式是高阶API模式，这种模式需要我们编写少量代码，才能完成一个目标检测任务。  \n",
    "\n",
    "本次实验，我们就来讲解一下如何调用PaddleX高阶API 来完成我们的目标检测项目。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验目标\n",
    "\n",
    "> - 掌握paddleX进行目标检测任务的工作流程\n",
    "> - 掌握本实验的核心代码\n",
    "\n",
    "## 实验内容\n",
    "\n",
    "### 1.准备阶段\n",
    "\n",
    "### 1.1 数据准备  \n",
    "\n",
    "对于本次实验的数据集，存放位置在data目录下。我们已经提前完成了标注，标注格式为VOC格式，标注之后生成了两个文件夹：Annotations和JPEGImages。其中Annotations用来存储数据集的xml文件；JPEGImages用来存储数据集的图片。\n",
    " \n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/ad198d705b3d42fb8e7f79468b9e0b90744712806ca54910a9171256bf853a9a\" width=300></center>\n",
    "<center>数据集结构</center>\n",
    "\n",
    "PaddleX在训练前需要导入数据集，这就需要解析图片与xml标注文件的路径，因为PaddleX的高阶API对数据集的路径要求比较严格，所以我们首先要生成几个txt文件，来向paddleX明确图片与xml文件的路径，这样，数据集才能被paddleX解析并加载。\n",
    "\n",
    "本次数据集一共两个类别：fire、smoke\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:57:26.456805Z",
     "iopub.status.busy": "2022-05-14T01:57:26.456635Z",
     "iopub.status.idle": "2022-05-14T01:57:26.474234Z",
     "shell.execute_reply": "2022-05-14T01:57:26.473732Z",
     "shell.execute_reply.started": "2022-05-14T01:57:26.456784Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and val size 437\r\n",
      "train size 356\r\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "说明：\n",
    "       本文件的作用是解析数据集中图片与xml文件的路径；\n",
    "       运行这个文件会生成四个个txt文件：\n",
    "            labels.txt用来存储数据集中瑕疵的各个类别名称；\n",
    "            train_list.txt用来存储训练集的图片，及对应的xml文件路径；\n",
    "            val_list.txt用来存储验证集的图片，及对应的xml文件路径；\n",
    "            test_list.txt用来存储测试集的图片，及对应的xml文件路径；\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import os       # OS模块含有文件操作的功能，对数据集读取时会用到该模块。\n",
    "import random   # 导入 random 模块，用于随时生成数据\n",
    "random.seed(0)  # 当我们设置相同的seed，每次生成的随机数相同\n",
    "\n",
    "num_class = ['fire','smoke']  #数据集中所有类名\n",
    "\n",
    "xmlfilepath=r'./traffic_lights/Annotations'   #xmlfilepath存储Annotations文件夹的路径\n",
    "saveBasePath=r\"./traffic_lights/\"             #saveBasePath用来存储生成的txt文件路径\n",
    "trainval_percent=0.98          #train+val数据集占总数据集98%，test占总数据集2%（主要用于进一步拆分数据集，属于中间变量）\n",
    "train_percent=0.8              #train占数据集80%\n",
    "\n",
    "temp_xml = os.listdir(xmlfilepath)  #temp_xml存储Annotations文件夹里的文件\n",
    "total_xml = []                      #total_xml存储后缀为xml的文件\n",
    "for xml in temp_xml:                #对于Annotations文件夹里的所有文件，如果文件是xml文件，将文件加入到total_xml中\n",
    "    if xml.endswith(\".xml\"):\n",
    "        total_xml.append(xml)\n",
    "\n",
    "num=len(total_xml)                  #num为xml文件的个数\n",
    "list=range(num)                     #list为所有xml文件的序号（0，1，2，3，...，num)\n",
    "tv=int(num*trainval_percent)        #tv为train+val数据集的个数\n",
    "tr=int(num*train_percent)             #tr为tarin数据集的个数\n",
    "trainval= random.sample(list,tv)    #trainval存储train数据集和val数据集的文件序号\n",
    "train=random.sample(trainval,tr)    #train存储train数据集的文件序号\n",
    " \n",
    "print(\"train and val size\",tv)                                      #检查train+val数据集个数\n",
    "print(\"train size\",tr)                                              #检查train数据集个数\n",
    "ftest = open(os.path.join(saveBasePath,'test_list.txt'), 'w')       #创建一个file对象来将test的数据名写入test_list.txt文件\n",
    "ftrain = open(os.path.join(saveBasePath,'train_list.txt'), 'w')     #创建一个file对象来将train数据名写入train_list.txt文件\n",
    "fval = open(os.path.join(saveBasePath,'val_list.txt'), 'w')         #创建一个file对象来将val数据名写入val_list.txt\n",
    "flabel = open(os.path.join(saveBasePath,'labels.txt'), 'w') #创建一个file对象来将label数据名写入labels.txt\n",
    "for i in num_class:                                         #遍历所有的label名称\n",
    "    flabel.write(i + \"\\n\")                                  #将名称写入文件\n",
    "flabel.close()                                              #关闭labels.txt文件\n",
    "\n",
    "for i  in list:                         #遍历所有xml文件\n",
    "    name=total_xml[i][:-4]+'\\n'         #name存储这一行文件内容\n",
    "    if i in trainval:                   #该文件在train+val数据集中，进一步检测是train数据集还是val数据集\n",
    "        if i in train:                  #该文件在train数据集中，将其写入train_list.txt文件\n",
    "            ftrain.write(\"Images/\" + name.strip().split(\".\")[0] + \".jpg\" + \" \" + \"Annotations/\" + name.strip().split(\".\")[0] + \".xml\" + \"\\n\")\n",
    "        else:                           #该文件在val数据集中，将其写入val_list.txt文件 \n",
    "            fval.write(\"Images/\" + name.strip().split(\".\")[0] + \".jpg\" + \" \" + \"Annotations/\" + name.strip().split(\".\")[0] + \".xml\" + \"\\n\")\n",
    "    else:                               #该文件在test数据集中，将其写入test_list.txt文件\n",
    "        ftest.write(\"Images/\" + name.strip().split(\".\")[0] + \".jpg\" + \" \" + \"Annotations/\" + name.strip().split(\".\")[0] + \".xml\" + \"\\n\")\n",
    "             \n",
    "ftrain.close()          #关闭train_list.txt文件\n",
    "fval.close()            #关闭val_list.txt文件\n",
    "ftest .close()          #关闭test_list.txt文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的代码后，就会生成四个txt文件：  \n",
    "\n",
    "> - labels.txt用来存储数据集中瑕疵的各个类别名称；\n",
    "> - train_list.txt用来存储训练集的图片，及对应的xml文件路径；\n",
    "> - val_list.txt用来存储验证集的图片，及对应的xml文件路径；\n",
    "> - test_list.txt用来存储测试集的图片，及对应的xml文件路径；  \n",
    "\n",
    "\n",
    "通过这四个文件，数据集已经被分成了训练集（train）、验证集（val）、测试集（test）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 数据预处理  \n",
    "\n",
    "为了训练出一个效果更好的模型，在数据集加载进网络之前，一般要对数据进行预处理，比如改变一下输入图片的尺寸，使得图片更适合网络的运算。或者，我们也可以用一些数据增强技术，来扩充我们的数据集（说明：对于数据增强的原理，我们会在后面课程中详细讲解）。paddleX中定义了一个图像处理流程transforms，在transforms中，我们可以定义数据集的预处理方式，也可以定义数据增强的方式，使用起来非常方便。如下代码可实现数据预处理：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:57:26.475136Z",
     "iopub.status.busy": "2022-05-14T01:57:26.474963Z",
     "iopub.status.idle": "2022-05-14T01:57:34.214801Z",
     "shell.execute_reply": "2022-05-14T01:57:34.214127Z",
     "shell.execute_reply.started": "2022-05-14T01:57:26.475116Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05-14 09:57:28 MainThread @utils.py:79] WRN paddlepaddle version: 2.3.0-rc0. The dynamic graph version of PARL is under development, not fully tested and supported\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/parl/remote/communication.py:38: DeprecationWarning: 'pyarrow.default_serialization_context' is deprecated as of 2.0.0 and will be removed in a future version. Use pickle or the pyarrow IPC functionality instead.\r\n",
      "  context = pyarrow.default_serialization_context()\r\n",
      "2022-05-14 09:57:29,336-WARNING: type object 'QuantizationTransformPass' has no attribute '_supported_quantizable_op_type'\r\n",
      "2022-05-14 09:57:29,338-WARNING: If you want to use training-aware and post-training quantization, please use Paddle >= 1.8.4 or develop version\r\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "说明：\n",
    "       本文件展示了paddleX高阶API的使用过程；\n",
    "       本文件通过调用paddleX高阶API可以实现数据导入、模型训练、模型预测等工作；\n",
    "   \n",
    "'''\n",
    "\n",
    "# 导入matplotlib绘图库\n",
    "import matplotlib\n",
    "#在导入matplotlib库后，且在matplotlib.pyplot库被导入前加“matplotlib.use(‘agg’)”语句，可以使得在PyCharm中不显示绘图。\n",
    "matplotlib.use('Agg') \n",
    "\n",
    "# OS模块含有文件操作的功能，对数据集读取时会用到该模块。\n",
    "import os\n",
    "# 设置使用0号GPU卡（如无GPU，执行此代码后仍然会使用CPU训练模型）\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#导入paddleX的高阶API\n",
    "import paddlex as pdx\n",
    "\n",
    "#定义图像处理流程transforms\n",
    "#定义训练数据的预处理方式，为了模型效果更好，可以对训练数据进行数据增强\n",
    "#如下代码中，训练过程使用了MixupImage、RandomDistort、RandomExpand、RandomCrop、Resize和RandomHorizontalFlip共6种数据增强方式\n",
    "\n",
    "#from paddlex.det import transforms #这是paddlex 1.3的API\n",
    "from paddlex import transforms #这是paddlex 2的API\n",
    "\n",
    "#对目标检测任务的数据进行操作。可以利用Compose类将图像预处理/增强操作进行组合\n",
    "train_transforms = transforms.Compose([\n",
    "    ##对图像进行mixup操作，可以使模型训练时数据得以增强，目前仅YOLOv3模型支持该操作\n",
    "    #在前mixup_epoch轮使用mixup增强操作；当该参数为-1时，该策略不会生效。默认为-1\n",
    "    #transforms.MixupImage(mixup_epoch=250), \n",
    "    #以一定的概率对图像进行随机像素内容变换，以达到数据增强的目的\n",
    "    transforms.RandomDistort(),\n",
    "    #随机扩张图像，以达到数据增强的目的\n",
    "    transforms.RandomExpand(),\n",
    "    #随机裁剪图像，以达到数据增强的目的\n",
    "    transforms.RandomCrop(),\n",
    "    #调整图像大小（resize）\n",
    "    #target_size 表示 短边目标长度。默认为608。\n",
    "    transforms.Resize(target_size=608, interp='RANDOM'),\n",
    "    #以一定的概率对图像进行随机水平翻转，以达到数据增强的目的\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    #对图像进行标准化\n",
    "    transforms.Normalize(), \n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码中，对于训练数据集，我们使用了MixupImage、RandomDistort、RandomExpand、RandomCrop和RandomHorizontalFlip共5种数据增强方式。我们利用Compose类将图像预处理/增强操作进行了组合。  \n",
    "\n",
    "同样的道理，对于验证集，我们也可以transforms.Compose()来进行数据的预处理，但是验证数据不需要数据增强，所以我们只需要改变一下图片的尺寸，把图片变成608×608大小，这个尺寸特别适合yolov3网络模型的运算。并且，我们也要对验证集的图片进行标准化操作。因此，去除数据增强相关代码后的代码如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:57:34.216994Z",
     "iopub.status.busy": "2022-05-14T01:57:34.216666Z",
     "iopub.status.idle": "2022-05-14T01:57:34.220673Z",
     "shell.execute_reply": "2022-05-14T01:57:34.220080Z",
     "shell.execute_reply.started": "2022-05-14T01:57:34.216965Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#定义验证集的预处理方式，验证数据不需要数据增强\n",
    "eval_transforms = transforms.Compose([\n",
    "    #调整图像大小（resize）\n",
    "    #target_size 表示 短边目标长度。默认为608。\n",
    "    transforms.Resize(target_size=608, interp='CUBIC'),\n",
    "    #对图像进行标准化\n",
    "    transforms.Normalize(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 加载数据  \n",
    "\n",
    "接下来，我们需要定义数据集Dataset，用来加载数据。在paddleX的高阶API中，目标检测可使用VOC和COCO两种数据加载方式。由于我们的数据集为VOC格式标注的，因此采用pdx.datasets.VOCDetection来加载训练数据集。\n",
    "同样的道理，我们也可以用pdx.datasets.VOCDetection来加载验证数据，代码如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:57:34.221922Z",
     "iopub.status.busy": "2022-05-14T01:57:34.221682Z",
     "iopub.status.idle": "2022-05-14T01:57:34.586261Z",
     "shell.execute_reply": "2022-05-14T01:57:34.585442Z",
     "shell.execute_reply.started": "2022-05-14T01:57:34.221899Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-14 09:57:34 [INFO]\tStarting to read file list from dataset...\r\n",
      "2022-05-14 09:57:34 [INFO]\t356 samples in file traffic_lights/train_list.txt, including 356 positive samples and 0 negative samples.\r\n",
      "creating index...\r\n",
      "index created!\r\n",
      "2022-05-14 09:57:34 [INFO]\tStarting to read file list from dataset...\r\n",
      "2022-05-14 09:57:34 [INFO]\t81 samples in file traffic_lights/val_list.txt, including 81 positive samples and 0 negative samples.\r\n",
      "creating index...\r\n",
      "index created!\r\n"
     ]
    }
   ],
   "source": [
    "#定义数据集Dataset，用来加载数据\n",
    "#目标检测可使用VOCDetection格式和COCODetection两种数据集，\n",
    "#此处由于我们的数据集为VOC格式，因此采用pdx.datasets.VOCDetection来加载数据集，\n",
    "train_dataset = pdx.datasets.VOCDetection(    \n",
    "    data_dir='traffic_lights',                 #数据集路径\n",
    "    file_list='traffic_lights/train_list.txt', #指向train_list.txt的路径，也即是训练集的路径\n",
    "    label_list='traffic_lights/labels.txt',    #指向labels.txt的路径\n",
    "    transforms=train_transforms,\n",
    "    shuffle=True)\n",
    "\n",
    "    \n",
    "eval_dataset = pdx.datasets.VOCDetection(\n",
    "    data_dir='traffic_lights',\n",
    "    file_list='traffic_lights/val_list.txt', #指向val_list.txt的路径，也即是验证集的路径\n",
    "    label_list='traffic_lights/labels.txt',  #指向labels.txt的路径\n",
    "    transforms=eval_transforms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "\n",
    "上面代码中buffer_size表示数据集中样本在预处理过程中队列的缓存长度，这个值如果设置的过大，则会出现memory manager的问题，程序会报错。所以这个值不能设置过大。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 训练与评估  \n",
    "\n",
    "准备阶段的工作完成之后，我们就可以进行模型的训练了， 代码如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-14T01:57:34.587372Z",
     "iopub.status.busy": "2022-05-14T01:57:34.587195Z",
     "iopub.status.idle": "2022-05-14T02:21:25.291991Z",
     "shell.execute_reply": "2022-05-14T02:21:25.289492Z",
     "shell.execute_reply.started": "2022-05-14T01:57:34.587348Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0514 09:57:34.591156   246 gpu_context.cc:244] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\r\n",
      "W0514 09:57:34.596359   246 gpu_context.cc:272] device: 0, cuDNN Version: 7.6.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-14 09:57:38 [INFO]\tLoading pretrained model from saved_model/pretrain/yolov3_darknet53_270e_coco.pdparams\r\n",
      "2022-05-14 09:57:38 [WARNING]\t[SKIP] Shape of pretrained params yolo_head.yolo_output.0.weight doesn't match.(Pretrained: [255, 1024, 1, 1], Actual: [21, 1024, 1, 1])\r\n",
      "2022-05-14 09:57:38 [WARNING]\t[SKIP] Shape of pretrained params yolo_head.yolo_output.0.bias doesn't match.(Pretrained: [255], Actual: [21])\r\n",
      "2022-05-14 09:57:38 [WARNING]\t[SKIP] Shape of pretrained params yolo_head.yolo_output.1.weight doesn't match.(Pretrained: [255, 512, 1, 1], Actual: [21, 512, 1, 1])\r\n",
      "2022-05-14 09:57:38 [WARNING]\t[SKIP] Shape of pretrained params yolo_head.yolo_output.1.bias doesn't match.(Pretrained: [255], Actual: [21])\r\n",
      "2022-05-14 09:57:38 [WARNING]\t[SKIP] Shape of pretrained params yolo_head.yolo_output.2.weight doesn't match.(Pretrained: [255, 256, 1, 1], Actual: [21, 256, 1, 1])\r\n",
      "2022-05-14 09:57:38 [WARNING]\t[SKIP] Shape of pretrained params yolo_head.yolo_output.2.bias doesn't match.(Pretrained: [255], Actual: [21])\r\n",
      "2022-05-14 09:57:38 [INFO]\tThere are 360/366 variables loaded into YOLOv3.\r\n",
      "2022-05-14 09:58:05 [INFO]\t[TRAIN] Epoch=1/100, Step=10/22, loss_xy=2.852249, loss_wh=2.434420, loss_obj=39.672443, loss_cls=1.373317, loss=46.332432, lr=0.000125, time_each_step=2.56s, eta=1:33:44\r\n",
      "2022-05-14 09:58:30 [INFO]\t[TRAIN] Epoch=1/100, Step=20/22, loss_xy=2.908123, loss_wh=1.472659, loss_obj=15.857628, loss_cls=1.442825, loss=21.681234, lr=0.000125, time_each_step=2.57s, eta=1:33:42\r\n",
      "2022-05-14 09:58:34 [INFO]\t[TRAIN] Epoch 1 finished, loss_xy=2.906576, loss_wh=2.9016638, loss_obj=1108.8236, loss_cls=1.5234061, loss=1116.1553 .\r\n",
      "2022-05-14 09:58:58 [INFO]\t[TRAIN] Epoch=2/100, Step=8/22, loss_xy=2.815331, loss_wh=1.158541, loss_obj=13.582929, loss_cls=1.318342, loss=18.875143, lr=0.000125, time_each_step=2.77s, eta=1:40:36\r\n",
      "2022-05-14 09:59:22 [INFO]\t[TRAIN] Epoch=2/100, Step=18/22, loss_xy=2.719472, loss_wh=0.968635, loss_obj=6.835631, loss_cls=1.242689, loss=11.766427, lr=0.000125, time_each_step=2.36s, eta=1:25:19\r\n",
      "2022-05-14 09:59:32 [INFO]\t[TRAIN] Epoch 2 finished, loss_xy=2.7944448, loss_wh=1.1224158, loss_obj=9.41083, loss_cls=1.3946772, loss=14.722366 .\r\n",
      "2022-05-14 09:59:47 [INFO]\t[TRAIN] Epoch=3/100, Step=6/22, loss_xy=2.735567, loss_wh=0.765416, loss_obj=5.765611, loss_cls=1.623456, loss=10.890050, lr=0.000125, time_each_step=2.54s, eta=1:31:8\r\n",
      "2022-05-14 10:00:11 [INFO]\t[TRAIN] Epoch=3/100, Step=16/22, loss_xy=2.720212, loss_wh=0.844128, loss_obj=11.285440, loss_cls=1.281907, loss=16.131687, lr=0.000125, time_each_step=2.37s, eta=1:24:53\r\n",
      "2022-05-14 10:00:27 [INFO]\t[TRAIN] Epoch 3 finished, loss_xy=2.7715585, loss_wh=0.86041886, loss_obj=8.013503, loss_cls=1.441533, loss=13.087014 .\r\n",
      "2022-05-14 10:00:38 [INFO]\t[TRAIN] Epoch=4/100, Step=4/22, loss_xy=2.794840, loss_wh=0.686883, loss_obj=15.675466, loss_cls=1.361134, loss=20.518322, lr=0.000125, time_each_step=2.72s, eta=1:36:39\r\n",
      "2022-05-14 10:01:01 [INFO]\t[TRAIN] Epoch=4/100, Step=14/22, loss_xy=2.831705, loss_wh=0.772137, loss_obj=7.645704, loss_cls=1.337485, loss=12.587030, lr=0.000125, time_each_step=2.26s, eta=1:20:4\r\n",
      "2022-05-14 10:01:20 [INFO]\t[TRAIN] Epoch 4 finished, loss_xy=2.7921746, loss_wh=0.82785416, loss_obj=10.07707, loss_cls=1.4093236, loss=15.106423 .\r\n",
      "2022-05-14 10:01:26 [INFO]\t[TRAIN] Epoch=5/100, Step=2/22, loss_xy=2.730960, loss_wh=0.791993, loss_obj=11.121579, loss_cls=1.471475, loss=16.116007, lr=0.000125, time_each_step=2.48s, eta=1:27:28\r\n",
      "2022-05-14 10:01:50 [INFO]\t[TRAIN] Epoch=5/100, Step=12/22, loss_xy=2.783066, loss_wh=0.840436, loss_obj=6.089327, loss_cls=1.472101, loss=11.184931, lr=0.000125, time_each_step=2.38s, eta=1:23:41\r\n",
      "2022-05-14 10:02:16 [INFO]\t[TRAIN] Epoch=5/100, Step=22/22, loss_xy=2.788544, loss_wh=0.781062, loss_obj=5.996872, loss_cls=1.380236, loss=10.946713, lr=0.000125, time_each_step=2.61s, eta=1:31:1\r\n",
      "2022-05-14 10:02:16 [INFO]\t[TRAIN] Epoch 5 finished, loss_xy=2.7659013, loss_wh=0.75137544, loss_obj=7.0720596, loss_cls=1.400685, loss=11.990021 .\r\n",
      "2022-05-14 10:02:40 [INFO]\t[TRAIN] Epoch=6/100, Step=10/22, loss_xy=2.848881, loss_wh=0.914737, loss_obj=7.128802, loss_cls=1.367281, loss=12.259701, lr=0.000125, time_each_step=2.34s, eta=1:21:12\r\n",
      "2022-05-14 10:03:03 [INFO]\t[TRAIN] Epoch=6/100, Step=20/22, loss_xy=2.750181, loss_wh=0.841775, loss_obj=4.381859, loss_cls=1.423026, loss=9.396841, lr=0.000125, time_each_step=2.32s, eta=1:20:12\r\n",
      "2022-05-14 10:03:08 [INFO]\t[TRAIN] Epoch 6 finished, loss_xy=2.7645493, loss_wh=0.80254096, loss_obj=6.679385, loss_cls=1.3931094, loss=11.639585 .\r\n",
      "2022-05-14 10:03:30 [INFO]\t[TRAIN] Epoch=7/100, Step=8/22, loss_xy=2.724658, loss_wh=0.814864, loss_obj=11.037599, loss_cls=1.436715, loss=16.013836, lr=0.000125, time_each_step=2.75s, eta=1:34:37\r\n",
      "2022-05-14 10:03:55 [INFO]\t[TRAIN] Epoch=7/100, Step=18/22, loss_xy=2.771841, loss_wh=0.697316, loss_obj=4.086379, loss_cls=1.430324, loss=8.985860, lr=0.000125, time_each_step=2.43s, eta=1:23:6\r\n",
      "2022-05-14 10:04:04 [INFO]\t[TRAIN] Epoch 7 finished, loss_xy=2.7555208, loss_wh=0.7738566, loss_obj=6.8698397, loss_cls=1.3915544, loss=11.790771 .\r\n",
      "2022-05-14 10:04:21 [INFO]\t[TRAIN] Epoch=8/100, Step=6/22, loss_xy=2.725867, loss_wh=1.026775, loss_obj=3.540218, loss_cls=1.314273, loss=8.607134, lr=0.000125, time_each_step=2.64s, eta=1:30:1\r\n",
      "2022-05-14 10:04:46 [INFO]\t[TRAIN] Epoch=8/100, Step=16/22, loss_xy=2.733081, loss_wh=0.774398, loss_obj=4.834825, loss_cls=1.365545, loss=9.707850, lr=0.000125, time_each_step=2.5s, eta=1:24:43\r\n",
      "2022-05-14 10:04:58 [INFO]\t[TRAIN] Epoch 8 finished, loss_xy=2.7665687, loss_wh=0.8215927, loss_obj=3.996859, loss_cls=1.3927355, loss=8.977756 .\r\n",
      "2022-05-14 10:05:09 [INFO]\t[TRAIN] Epoch=9/100, Step=4/22, loss_xy=2.744043, loss_wh=0.582534, loss_obj=3.545063, loss_cls=1.428599, loss=8.300241, lr=0.000125, time_each_step=2.27s, eta=1:16:38\r\n",
      "2022-05-14 10:05:35 [INFO]\t[TRAIN] Epoch=9/100, Step=14/22, loss_xy=2.753999, loss_wh=0.846599, loss_obj=3.578892, loss_cls=1.416724, loss=8.596214, lr=0.000125, time_each_step=2.62s, eta=1:27:54\r\n",
      "2022-05-14 10:05:55 [INFO]\t[TRAIN] Epoch 9 finished, loss_xy=2.7632828, loss_wh=0.766584, loss_obj=3.1623983, loss_cls=1.3634393, loss=8.055705 .\r\n",
      "2022-05-14 10:06:01 [INFO]\t[TRAIN] Epoch=10/100, Step=2/22, loss_xy=2.757341, loss_wh=0.785017, loss_obj=2.758569, loss_cls=1.467724, loss=7.768650, lr=0.000125, time_each_step=2.58s, eta=1:26:18\r\n",
      "2022-05-14 10:06:26 [INFO]\t[TRAIN] Epoch=10/100, Step=12/22, loss_xy=2.792080, loss_wh=0.736261, loss_obj=2.995102, loss_cls=1.359379, loss=7.882821, lr=0.000125, time_each_step=2.45s, eta=1:21:27\r\n",
      "2022-05-14 10:06:51 [INFO]\t[TRAIN] Epoch=10/100, Step=22/22, loss_xy=2.777954, loss_wh=0.594074, loss_obj=2.860592, loss_cls=1.363961, loss=7.596581, lr=0.000125, time_each_step=2.48s, eta=1:22:4\r\n",
      "2022-05-14 10:06:51 [INFO]\t[TRAIN] Epoch 10 finished, loss_xy=2.7508414, loss_wh=0.7621674, loss_obj=3.1024435, loss_cls=1.3653897, loss=7.9808426 .\r\n",
      "2022-05-14 10:07:16 [INFO]\t[TRAIN] Epoch=11/100, Step=10/22, loss_xy=2.745302, loss_wh=0.701820, loss_obj=2.880324, loss_cls=1.259468, loss=7.586914, lr=0.000125, time_each_step=2.56s, eta=1:24:10\r\n",
      "2022-05-14 10:07:40 [INFO]\t[TRAIN] Epoch=11/100, Step=20/22, loss_xy=2.768811, loss_wh=0.862778, loss_obj=2.981967, loss_cls=1.329079, loss=7.942636, lr=0.000125, time_each_step=2.36s, eta=1:17:28\r\n",
      "2022-05-14 10:07:45 [INFO]\t[TRAIN] Epoch 11 finished, loss_xy=2.7496016, loss_wh=0.7851295, loss_obj=2.8450437, loss_cls=1.3773494, loss=7.7571244 .\r\n",
      "2022-05-14 10:08:05 [INFO]\t[TRAIN] Epoch=12/100, Step=8/22, loss_xy=2.744050, loss_wh=0.915446, loss_obj=3.297155, loss_cls=1.366632, loss=8.323284, lr=0.000125, time_each_step=2.47s, eta=1:20:40\r\n",
      "2022-05-14 10:08:32 [INFO]\t[TRAIN] Epoch=12/100, Step=18/22, loss_xy=2.768736, loss_wh=0.900136, loss_obj=2.241945, loss_cls=1.369342, loss=7.280159, lr=0.000125, time_each_step=2.71s, eta=1:27:45\r\n",
      "2022-05-14 10:08:42 [INFO]\t[TRAIN] Epoch 12 finished, loss_xy=2.7500823, loss_wh=0.73754466, loss_obj=2.765843, loss_cls=1.363, loss=7.6164703 .\r\n",
      "2022-05-14 10:09:01 [INFO]\t[TRAIN] Epoch=13/100, Step=6/22, loss_xy=2.731816, loss_wh=0.894094, loss_obj=2.966784, loss_cls=1.371178, loss=7.963872, lr=0.000125, time_each_step=2.88s, eta=1:32:56\r\n",
      "2022-05-14 10:09:27 [INFO]\t[TRAIN] Epoch=13/100, Step=16/22, loss_xy=2.755985, loss_wh=0.722194, loss_obj=2.554065, loss_cls=1.285952, loss=7.318196, lr=0.000125, time_each_step=2.65s, eta=1:24:58\r\n",
      "2022-05-14 10:09:41 [INFO]\t[TRAIN] Epoch 13 finished, loss_xy=2.7435312, loss_wh=0.75048214, loss_obj=2.6398573, loss_cls=1.3653623, loss=7.499233 .\r\n",
      "2022-05-14 10:09:53 [INFO]\t[TRAIN] Epoch=14/100, Step=4/22, loss_xy=2.723078, loss_wh=0.709239, loss_obj=3.427597, loss_cls=1.320661, loss=8.180575, lr=0.000125, time_each_step=2.53s, eta=1:20:43\r\n",
      "2022-05-14 10:10:19 [INFO]\t[TRAIN] Epoch=14/100, Step=14/22, loss_xy=2.786717, loss_wh=0.745031, loss_obj=2.592413, loss_cls=1.360666, loss=7.484828, lr=0.000125, time_each_step=2.63s, eta=1:23:31\r\n",
      "2022-05-14 10:10:39 [INFO]\t[TRAIN] Epoch 14 finished, loss_xy=2.744346, loss_wh=0.7438574, loss_obj=2.4893134, loss_cls=1.3713956, loss=7.348912 .\r\n",
      "2022-05-14 10:10:45 [INFO]\t[TRAIN] Epoch=15/100, Step=2/22, loss_xy=2.736220, loss_wh=0.908731, loss_obj=2.493775, loss_cls=1.361557, loss=7.500283, lr=0.000125, time_each_step=2.56s, eta=1:20:57\r\n",
      "2022-05-14 10:11:11 [INFO]\t[TRAIN] Epoch=15/100, Step=12/22, loss_xy=2.734343, loss_wh=0.602985, loss_obj=2.403188, loss_cls=1.381510, loss=7.122025, lr=0.000125, time_each_step=2.61s, eta=1:21:53\r\n",
      "2022-05-14 10:11:36 [INFO]\t[TRAIN] Epoch=15/100, Step=22/22, loss_xy=2.715766, loss_wh=0.533015, loss_obj=1.842065, loss_cls=1.409677, loss=6.500524, lr=0.000125, time_each_step=2.49s, eta=1:18:0\r\n",
      "2022-05-14 10:11:36 [INFO]\t[TRAIN] Epoch 15 finished, loss_xy=2.732542, loss_wh=0.74597013, loss_obj=2.453994, loss_cls=1.3494215, loss=7.2819276 .\r\n",
      "2022-05-14 10:11:36 [WARNING]\tDetector only supports single card evaluation with batch_size=1 during evaluation, so batch_size is forcibly set to 1.\r\n",
      "2022-05-14 10:11:37 [INFO]\tStart to evaluate(total_samples=81, total_steps=81)...\r\n",
      "2022-05-14 10:11:40 [INFO]\tAccumulating evaluatation results...\r\n",
      "2022-05-14 10:11:40 [INFO]\t[EVAL] Finished, Epoch=15, bbox_map=36.642804 .\r\n",
      "2022-05-14 10:11:42 [INFO]\tModel saved in saved_model/best_model.\r\n",
      "2022-05-14 10:11:42 [INFO]\tCurrent evaluated best model on eval_dataset is epoch_15, bbox_map=36.64280448737691\r\n",
      "2022-05-14 10:11:44 [INFO]\tModel saved in saved_model/epoch_15.\r\n",
      "2022-05-14 10:12:12 [INFO]\t[TRAIN] Epoch=16/100, Step=10/22, loss_xy=2.690579, loss_wh=0.733086, loss_obj=2.365052, loss_cls=1.392211, loss=7.180928, lr=0.000125, time_each_step=2.81s, eta=1:27:47\r\n",
      "2022-05-14 10:12:36 [INFO]\t[TRAIN] Epoch=16/100, Step=20/22, loss_xy=2.681304, loss_wh=0.659519, loss_obj=2.297357, loss_cls=1.407822, loss=7.046000, lr=0.000125, time_each_step=2.45s, eta=1:16:14\r\n",
      "2022-05-14 10:12:41 [INFO]\t[TRAIN] Epoch 16 finished, loss_xy=2.7216213, loss_wh=0.74472964, loss_obj=2.266014, loss_cls=1.3634442, loss=7.0958095 .\r\n",
      "2022-05-14 10:13:01 [INFO]\t[TRAIN] Epoch=17/100, Step=8/22, loss_xy=2.717420, loss_wh=0.720355, loss_obj=2.149990, loss_cls=1.390894, loss=6.978659, lr=0.000125, time_each_step=2.47s, eta=1:16:20\r\n",
      "2022-05-14 10:13:24 [INFO]\t[TRAIN] Epoch=17/100, Step=18/22, loss_xy=2.664125, loss_wh=0.813412, loss_obj=1.839001, loss_cls=1.318540, loss=6.635079, lr=0.000125, time_each_step=2.32s, eta=1:11:12\r\n",
      "2022-05-14 10:13:33 [INFO]\t[TRAIN] Epoch 17 finished, loss_xy=2.7293377, loss_wh=0.77649015, loss_obj=2.2587237, loss_cls=1.3429668, loss=7.107519 .\r\n",
      "2022-05-14 10:13:51 [INFO]\t[TRAIN] Epoch=18/100, Step=6/22, loss_xy=2.756001, loss_wh=0.883212, loss_obj=2.371311, loss_cls=1.349999, loss=7.360522, lr=0.000125, time_each_step=2.67s, eta=1:21:25\r\n",
      "2022-05-14 10:14:15 [INFO]\t[TRAIN] Epoch=18/100, Step=16/22, loss_xy=2.707730, loss_wh=0.642063, loss_obj=2.424656, loss_cls=1.308190, loss=7.082640, lr=0.000125, time_each_step=2.35s, eta=1:11:29\r\n",
      "2022-05-14 10:14:30 [INFO]\t[TRAIN] Epoch 18 finished, loss_xy=2.7205591, loss_wh=0.75121427, loss_obj=2.3120992, loss_cls=1.3325157, loss=7.116388 .\r\n",
      "2022-05-14 10:14:41 [INFO]\t[TRAIN] Epoch=19/100, Step=4/22, loss_xy=2.739547, loss_wh=0.606385, loss_obj=2.340775, loss_cls=1.307422, loss=6.994129, lr=0.000125, time_each_step=2.63s, eta=1:19:26\r\n",
      "2022-05-14 10:15:04 [INFO]\t[TRAIN] Epoch=19/100, Step=14/22, loss_xy=2.728606, loss_wh=0.799568, loss_obj=2.220016, loss_cls=1.331716, loss=7.079905, lr=0.000125, time_each_step=2.28s, eta=1:8:33\r\n",
      "2022-05-14 10:15:25 [INFO]\t[TRAIN] Epoch 19 finished, loss_xy=2.7187107, loss_wh=0.7545256, loss_obj=2.1997948, loss_cls=1.3376378, loss=7.010668 .\r\n",
      "2022-05-14 10:15:32 [INFO]\t[TRAIN] Epoch=20/100, Step=2/22, loss_xy=2.765099, loss_wh=0.726481, loss_obj=2.060964, loss_cls=1.234264, loss=6.786807, lr=0.000125, time_each_step=2.76s, eta=1:22:18\r\n",
      "2022-05-14 10:15:55 [INFO]\t[TRAIN] Epoch=20/100, Step=12/22, loss_xy=2.738913, loss_wh=0.626720, loss_obj=1.974150, loss_cls=1.327621, loss=6.667404, lr=0.000125, time_each_step=2.33s, eta=1:9:19\r\n",
      "2022-05-14 10:16:18 [INFO]\t[TRAIN] Epoch=20/100, Step=22/22, loss_xy=2.705183, loss_wh=0.786097, loss_obj=2.209304, loss_cls=1.347406, loss=7.047991, lr=0.000125, time_each_step=2.27s, eta=1:7:4\r\n",
      "2022-05-14 10:16:18 [INFO]\t[TRAIN] Epoch 20 finished, loss_xy=2.7211213, loss_wh=0.7186563, loss_obj=2.1812265, loss_cls=1.3338991, loss=6.954904 .\r\n",
      "2022-05-14 10:16:42 [INFO]\t[TRAIN] Epoch=21/100, Step=10/22, loss_xy=2.711749, loss_wh=0.620667, loss_obj=2.004286, loss_cls=1.416403, loss=6.753105, lr=0.000125, time_each_step=2.43s, eta=1:11:32\r\n",
      "2022-05-14 10:17:07 [INFO]\t[TRAIN] Epoch=21/100, Step=20/22, loss_xy=2.705894, loss_wh=0.753231, loss_obj=3.074443, loss_cls=1.332148, loss=7.865716, lr=0.000125, time_each_step=2.44s, eta=1:11:22\r\n",
      "2022-05-14 10:17:13 [INFO]\t[TRAIN] Epoch 21 finished, loss_xy=2.719019, loss_wh=0.7680649, loss_obj=2.1819634, loss_cls=1.3277171, loss=6.9967656 .\r\n",
      "2022-05-14 10:17:34 [INFO]\t[TRAIN] Epoch=22/100, Step=8/22, loss_xy=2.614080, loss_wh=0.661324, loss_obj=1.797418, loss_cls=1.222551, loss=6.295373, lr=0.000125, time_each_step=2.66s, eta=1:17:16\r\n",
      "2022-05-14 10:17:57 [INFO]\t[TRAIN] Epoch=22/100, Step=18/22, loss_xy=2.706032, loss_wh=0.709494, loss_obj=2.296771, loss_cls=1.247143, loss=6.959440, lr=0.000125, time_each_step=2.3s, eta=1:6:26\r\n",
      "2022-05-14 10:18:05 [INFO]\t[TRAIN] Epoch 22 finished, loss_xy=2.7090483, loss_wh=0.7593746, loss_obj=1.982945, loss_cls=1.3242406, loss=6.775609 .\r\n",
      "2022-05-14 10:18:21 [INFO]\t[TRAIN] Epoch=23/100, Step=6/22, loss_xy=2.705894, loss_wh=0.781422, loss_obj=1.606577, loss_cls=1.363453, loss=6.457347, lr=0.000125, time_each_step=2.42s, eta=1:9:28\r\n",
      "2022-05-14 10:18:45 [INFO]\t[TRAIN] Epoch=23/100, Step=16/22, loss_xy=2.705588, loss_wh=0.814951, loss_obj=2.100261, loss_cls=1.353651, loss=6.974451, lr=0.000125, time_each_step=2.42s, eta=1:9:5\r\n",
      "2022-05-14 10:18:58 [INFO]\t[TRAIN] Epoch 23 finished, loss_xy=2.7037141, loss_wh=0.7368835, loss_obj=2.0620685, loss_cls=1.3187066, loss=6.821373 .\r\n",
      "2022-05-14 10:19:10 [INFO]\t[TRAIN] Epoch=24/100, Step=4/22, loss_xy=2.709553, loss_wh=0.629204, loss_obj=1.867706, loss_cls=1.405669, loss=6.612132, lr=0.000125, time_each_step=2.43s, eta=1:9:7\r\n",
      "2022-05-14 10:19:33 [INFO]\t[TRAIN] Epoch=24/100, Step=14/22, loss_xy=2.767593, loss_wh=0.708299, loss_obj=2.205281, loss_cls=1.316427, loss=6.997599, lr=0.000125, time_each_step=2.36s, eta=1:6:43\r\n",
      "2022-05-14 10:19:53 [INFO]\t[TRAIN] Epoch 24 finished, loss_xy=2.7125032, loss_wh=0.7199763, loss_obj=2.1109154, loss_cls=1.3262712, loss=6.8696656 .\r\n",
      "2022-05-14 10:19:59 [INFO]\t[TRAIN] Epoch=25/100, Step=2/22, loss_xy=2.717866, loss_wh=1.005516, loss_obj=2.984003, loss_cls=1.343093, loss=8.050478, lr=0.000125, time_each_step=2.56s, eta=1:11:38\r\n",
      "2022-05-14 10:20:23 [INFO]\t[TRAIN] Epoch=25/100, Step=12/22, loss_xy=2.743196, loss_wh=0.645508, loss_obj=2.097167, loss_cls=1.289102, loss=6.774972, lr=0.000125, time_each_step=2.41s, eta=1:7:8\r\n",
      "2022-05-14 10:20:44 [INFO]\t[TRAIN] Epoch=25/100, Step=22/22, loss_xy=2.679663, loss_wh=0.550785, loss_obj=2.332441, loss_cls=1.370446, loss=6.933335, lr=0.000125, time_each_step=2.11s, eta=0:58:23\r\n",
      "2022-05-14 10:20:44 [INFO]\t[TRAIN] Epoch 25 finished, loss_xy=2.6891222, loss_wh=0.7317839, loss_obj=2.188639, loss_cls=1.2964823, loss=6.9060283 .\r\n",
      "2022-05-14 10:21:13 [INFO]\t[TRAIN] Epoch=26/100, Step=10/22, loss_xy=2.710896, loss_wh=0.717290, loss_obj=1.847697, loss_cls=1.193088, loss=6.468971, lr=0.000125, time_each_step=2.85s, eta=1:18:17\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-33:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\r\n",
      "    self.run()\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/threading.py\", line 870, in run\r\n",
      "    self._target(*self._args, **self._kwargs)\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\", line 527, in _thread_loop\r\n",
      "    batch = self._get_data()\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\", line 664, in _get_data\r\n",
      "    batch.reraise()\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/worker.py\", line 169, in reraise\r\n",
      "    raise self.exc_type(msg)\r\n",
      "ValueError: DataLoader worker(0) caught ValueError with message:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/transforms/operators.py\", line 156, in apply_im\r\n",
      "    image = self.read_img(im_path)\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/transforms/operators.py\", line 151, in read_img\r\n",
      "    cv2.IMREAD_COLOR)\r\n",
      "KeyboardInterrupt\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/worker.py\", line 329, in _worker_loop\r\n",
      "    batch = fetcher.fetch(indices)\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/fetcher.py\", line 121, in fetch\r\n",
      "    data.append(self.dataset[idx])\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/datasets/voc.py\", line 324, in __getitem__\r\n",
      "    sample = self.transforms(sample)\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/transforms/operators.py\", line 119, in __call__\r\n",
      "    sample = self.decode_image(sample)\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/transforms/operators.py\", line 82, in __call__\r\n",
      "    sample = self.apply(sample)\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/transforms/operators.py\", line 189, in apply\r\n",
      "    sample['image'] = self.apply_im(sample['image'])\r\n",
      "  File \"/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/transforms/operators.py\", line 159, in apply_im\r\n",
      "    im_path))\r\n",
      "ValueError: Cannot read the image file traffic_lights/Images/green00063.jpg!\r\n",
      "\r\n",
      "\r\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_in_legacy_dygraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_next_var_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_restore_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemError\u001b[0m: (Fatal) Blocking queue is killed because the data reader raises an exception.\n  [Hint: Expected killed_ != true, but received killed_:1 == true:1.] (at /paddle/paddle/fluid/operators/reader/blocking_queue.h:166)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_246/2947238037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0msave_interval_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m        \u001b[0;31m#每隔多少epoch保存一次模型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'saved_model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#模型的保存位置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     use_vdl=True)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/models/detector.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, train_dataset, train_batch_size, eval_dataset, optimizer, save_interval_epochs, log_interval_steps, save_dir, pretrain_weights, learning_rate, warmup_steps, warmup_start_lr, lr_decay_epochs, lr_decay_gamma, metric, use_ema, early_stop, early_stop_patience, use_vdl, resume_checkpoint)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop_patience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             use_vdl=use_vdl)\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     def quant_aware_train(self,\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddlex/cv/models/base.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(self, num_epochs, train_dataset, train_batch_size, eval_dataset, save_interval_epochs, log_interval_steps, save_dir, ema, early_stop, early_stop_patience, use_vdl)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mstep_time_tic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnranks\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mddp_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dataloader/dataloader_iter.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_in_legacy_dygraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_next_var_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_restore_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#模型训练    \n",
    "#模型训练过程每间隔save_interval_epochs轮会保存一次模型在save_dir目录下，\n",
    "#同时在保存的过程中也会在验证数据集上计算相关指标\n",
    "num_classes = len(train_dataset.labels) #num_classes表示瑕疵的类别数\n",
    "#调用paddleX高阶API中的yolov3算法，其backbone为DarkNet53\n",
    "model = pdx.det.YOLOv3(num_classes=num_classes, backbone='DarkNet53')   \n",
    "#model = pdx.det.YOLOv3(num_classes=num_classes, backbone='MobileNetV1')\n",
    "\n",
    "model.train(    \n",
    "    num_epochs=100, #训练的epoch数\n",
    "    train_dataset=train_dataset,    #加载增强后的训练数据\n",
    "    train_batch_size=16,             #每一批次大小\n",
    "    eval_dataset=eval_dataset,      #导入验证数据\n",
    "    learning_rate=0.000125,         #学习率\n",
    "    lr_decay_epochs=[50, 150],     #采用可变学习率，在这里规定学习率的范围，把epoch分为0-100、100-20、200-以上  三个范围，每个范围内采用不同的学习率\n",
    "    pretrain_weights='COCO',\n",
    "    save_interval_epochs=15,        #每隔多少epoch保存一次模型\n",
    "    save_dir='saved_model', #模型的保存位置\n",
    "    use_vdl=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-14T02:21:25.293285Z",
     "iopub.status.idle": "2022-05-14T02:21:25.293606Z",
     "shell.execute_reply": "2022-05-14T02:21:25.293460Z",
     "shell.execute_reply.started": "2022-05-14T02:21:25.293441Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(\n",
    "    eval_dataset = eval_dataset, \n",
    "    batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - 以上代码中，通过调用pdx.det.YOLOv3来构建一个基于YOLOv3算法的检测器。也就是说，在paddleX中，我们不需要再自己搭建yolov3算法的网络结构了，paddleX的API会帮我们搭建好。\n",
    "\n",
    "> - pdx.det.YOLOv3有一些参数需要注意。num_classes不需要包含背景类，比如咱们本次实验的待检测物体共3类，则num_classes设为3即可，这里与FasterRCNN/MaskRCNN有差别。\n",
    "\n",
    "\n",
    "> - lr_decay_epochs=[100, 200]表示我们采用可变学习率。[100, 200]规定了学习率的范围，表示把epoch分为“0-100、100-200、200-以上”三个范围，每个范围内采用不同的学习率。比如本次代码中，我们在0-100个epoch内，使用的learning_rate=0.000125，在100-200个epoch内，是学习率衰减0.1倍，变成0.0000125，当200个epoch以上时，学习率再次衰减0.1倍。\n",
    "\n",
    "\n",
    "> - 以上代码中，模型训练过程每间隔save_interval_epochs轮会保存一次模型在save_dir目录下，同时在保存的过程中也会在验证数据集上计算相关指标。训练过程会持续十几个小时，当模型评估的loss值不再减小，或者变化不大时，就可以停止训练了。训练过程的轮次epoch、损失值loss，学习率lr等信息变化，如图所示。\n",
    " \n",
    " <center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/877fc6ca58ea49619891a6f1056f469b375e43f2a4aa48b996ce5835808c4efe\" width=600></center>\n",
    "<center></center>\n",
    " \n",
    "可以看到，随着训练的加深，loss值逐渐在减小。\n",
    "模型训练结束后，通过语句eval_dataset=eval_dataset把验证集加载进模型，可以进行模型的评估。并且把评估结果输出在屏幕上，如下图所示。\n",
    "\n",
    " <center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/585ff8593a004631b7e50e32beebdfc9ed242bcade714775999b622e83ee745e\" width=600></center>\n",
    "<center></center>\n",
    "从图中最后一行代码，可以读出，在验证集上，当前最好的模型是epoch_10，预测框的mAP值为4.12。评估后，会把效果最好的模型保存下来，名字为best_model。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 预测展示\n",
    "经过漫长的模型训练和评估之后，我们保存好一个效果不错的模型，就可以用这个模型进行预测，并把预测结果可视化的展示出来。预测部份代码如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-14T02:21:25.295263Z",
     "iopub.status.idle": "2022-05-14T02:21:25.295559Z",
     "shell.execute_reply": "2022-05-14T02:21:25.295423Z",
     "shell.execute_reply.started": "2022-05-14T02:21:25.295404Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#预测阶段    \n",
    "import paddlex as pdx\n",
    "#加载训练好的模型\n",
    "model = pdx.load_model('saved_model/epoch_60')\n",
    "#选择待预测的图片\n",
    "\n",
    "#image_name = 'traffic_lights/Images/red00154.jpg'\n",
    "#image_name = 'traffic_lights/Images/red00208.jpg'\n",
    "#image_name = 'traffic_lights/Images/red00090.jpg'\n",
    "#image_name = 'traffic_lights/Images/green00082.jpg'\n",
    "#image_name = 'traffic_lights/Images/red00173.jpg'\n",
    "image_name = 'traffic_lights/Images/red00013.jpg'\n",
    "#image_name = 'traffic_lights/Images/green00138.jpg'\n",
    "#image_name = 'traffic_lights/Images/red00028.jpg'\n",
    "#image_name = 'traffic_lights/Images/green00094.jpg'\n",
    "\n",
    "\n",
    "#启动预测\n",
    "result = model.predict(image_name)\n",
    "#使用pdx.det.visualize将结果可视化，可视化结果将保存到save_dir目录下，\n",
    "#其中threshold代表Box的置信度阈值，将Box置信度低于该阈值的框过滤掉，不进行可视化。\n",
    "pdx.det.visualize(image_name, result, threshold=0.5, save_dir='./saved_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码中，我们使用pdx.det.visualize将预测的结果可视化展示出来，可视化结果是一张jpg图片，程序会将该图片保存到save_dir目录下。其中参数threshold代表Box的置信度阈值， 如果模型预测出的矩形框的置信度很低，低于这个threshold值，则这个预测框不会展示出来。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
